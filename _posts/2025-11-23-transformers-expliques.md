---
layout: post-interactive
title: "Les Transformers Expliqu√©s : De l'Attention √† ChatGPT"
date: 2025-11-23
author: "S√©bastien Sime"
categories: [Machine Learning, NLP]
tags: [transformers, attention, deep-learning, ia]
---

## Introduction

Les **Transformers** ont r√©volutionn√© le traitement du langage naturel (NLP) et sont √† la base de tous les grands mod√®les de langage modernes comme GPT, BERT, et bien d'autres. Introduits en 2017 par Vaswani et al. dans le c√©l√®bre article "Attention is All You Need", les Transformers ont remplac√© les architectures r√©currentes (RNN, LSTM) pour devenir la r√©f√©rence en NLP.

Dans cet article, nous allons explorer en d√©tail le fonctionnement des Transformers, en mettant l'accent sur le m√©canisme d'**attention** qui en est le c≈ìur.

## Le Probl√®me des Architectures R√©currentes

Avant les Transformers, les mod√®les de s√©quence utilisaient principalement des **r√©seaux de neurones r√©currents** (RNN) et des **LSTM**. Ces architectures avaient plusieurs limitations :

### Traitement S√©quentiel

Les RNN traitent les mots un par un, dans l'ordre. Cela pose deux probl√®mes majeurs :

1. **Parall√©lisation impossible** : On ne peut pas traiter plusieurs mots en m√™me temps, ce qui ralentit l'entra√Ænement
2. **D√©pendances longue distance** : Les informations du d√©but de la phrase peuvent se perdre quand on arrive √† la fin

### Gradient Vanishing

M√™me avec les LSTM, il est difficile de capturer des d√©pendances sur de tr√®s longues s√©quences. Le gradient a tendance √† dispara√Ætre lors de la r√©tropropagation √† travers de nombreux pas de temps.

## L'Id√©e R√©volutionnaire : L'Attention

Les Transformers proposent une solution √©l√©gante : **et si chaque mot pouvait directement regarder tous les autres mots de la phrase ?**

C'est exactement ce que fait le m√©canisme d'**auto-attention** (self-attention). Au lieu de traiter les mots s√©quentiellement, chaque mot calcule une "attention" sur tous les autres mots pour d√©terminer lesquels sont importants pour le comprendre.

### Exemple Concret

Prenons la phrase : **"Le chat mange la souris"**

Quand le mod√®le traite le mot "mange", il va calculer des scores d'attention pour tous les autres mots :

- **"Le"** : faible attention (article peu informatif)
- **"chat"** : forte attention (sujet de l'action)
- **"la"** : faible attention
- **"souris"** : forte attention (objet de l'action)

Le mot "mange" va donc se concentrer sur "chat" et "souris" pour mieux se comprendre dans le contexte.

## Le M√©canisme d'Auto-Attention (Self-Attention)

Le m√©canisme d'attention utilise trois concepts cl√©s : les **Query (Q)**, **Key (K)**, et **Value (V)**.

### Les Trois Matrices : Q, K, V

Pour chaque mot de la phrase, on calcule trois vecteurs :

1. **Query (Q)** : "Que cherche ce mot ?"
2. **Key (K)** : "Qu'est-ce que ce mot offre ?"
3. **Value (V)** : "Quelle information ce mot contient ?"

Ces trois vecteurs sont obtenus en multipliant l'embedding du mot par trois matrices de poids apprises pendant l'entra√Ænement.

### Calcul de l'Attention

Le score d'attention entre deux mots est calcul√© en trois √©tapes :

1. **Score** : Produit scalaire entre la Query d'un mot et la Key d'un autre
   ```
   score(mot_i, mot_j) = Q_i ¬∑ K_j
   ```

2. **Normalisation** : Division par la racine carr√©e de la dimension (pour stabiliser les gradients)
   ```
   score_normalized = score / sqrt(d_k)
   ```

3. **Softmax** : Conversion en probabilit√©s
   ```
   attention_weights = softmax(score_normalized)
   ```

4. **Weighted Sum** : Somme pond√©r√©e des Values
   ```
   output = sum(attention_weights * V)
   ```

### Formule Math√©matique

La formule compl√®te de l'attention est :

```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
```

O√π :
- `Q`, `K`, `V` sont les matrices Query, Key, Value
- `d_k` est la dimension des vecteurs Key
- `QK^T` est le produit matriciel donnant les scores d'attention

## Multi-Head Attention

Une des innovations cl√©s des Transformers est l'utilisation de **plusieurs t√™tes d'attention** en parall√®le.

### Pourquoi Plusieurs T√™tes ?

Chaque t√™te d'attention peut se sp√©cialiser dans la capture d'un type de relation diff√©rent :

- **T√™te 1** : Relations syntaxiques (sujet-verbe)
- **T√™te 2** : Relations s√©mantiques (synonymes, antonymes)
- **T√™te 3** : R√©f√©rences anaphoriques (pronoms ‚Üí noms)

### Fonctionnement

Au lieu d'avoir une seule attention, on en calcule 8 ou 12 en parall√®le (selon le mod√®le), chacune avec ses propres matrices Q, K, V. Les r√©sultats sont ensuite concat√©n√©s et projet√©s.

```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_O

o√π head_i = Attention(Q*W_Qi, K*W_Ki, V*W_Vi)
```

## Positional Encoding

Il y a un probl√®me : contrairement aux RNN, les Transformers ne traitent pas les mots dans l'ordre. La phrase "Le chat mange la souris" et "La souris mange le chat" produiraient le m√™me r√©sultat !

Pour r√©soudre ce probl√®me, on ajoute un **encodage positionnel** (positional encoding) aux embeddings des mots.

### Encodage Sinuso√Ødal

L'article original utilise des fonctions sinuso√Ødales pour encoder la position :

```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

O√π :
- `pos` est la position du mot dans la phrase
- `i` est la dimension
- `d_model` est la dimension du mod√®le

Cet encodage a l'avantage de permettre au mod√®le de g√©n√©raliser √† des s√©quences plus longues que celles vues pendant l'entra√Ænement.

## Architecture Compl√®te d'un Transformer

Un Transformer complet se compose de deux parties :

### Encoder

1. **Embeddings** + Positional Encoding
2. **Multi-Head Attention** (auto-attention)
3. **Add & Norm** (connexion r√©siduelle + normalisation)
4. **Feed-Forward Network** (deux couches lin√©aires avec ReLU)
5. **Add & Norm**

Ces blocs sont r√©p√©t√©s N fois (typiquement 6 ou 12 fois).

### Decoder

Similaire √† l'encoder, mais avec une couche suppl√©mentaire :

1. **Masked Multi-Head Attention** (pour √©viter de regarder le futur)
2. **Add & Norm**
3. **Multi-Head Attention** (attention crois√©e avec l'encoder)
4. **Add & Norm**
5. **Feed-Forward Network**
6. **Add & Norm**

## Applications des Transformers

Les Transformers sont utilis√©s dans de nombreuses t√¢ches :

### Traitement du Langage

- **BERT** : Pr√©-entra√Ænement bidirectionnel pour la compr√©hension
- **GPT** : G√©n√©ration de texte autoregressive
- **T5** : Approche text-to-text unifi√©e

### Au-del√† du Texte

- **Vision Transformers (ViT)** : Classification d'images
- **DALL-E** : G√©n√©ration d'images √† partir de texte
- **Whisper** : Reconnaissance vocale
- **AlphaFold** : Pr√©diction de structure de prot√©ines

## Avantages des Transformers

1. **Parall√©lisation** : Tous les mots sont trait√©s en m√™me temps
2. **D√©pendances longue distance** : Chaque mot peut directement interagir avec tous les autres
3. **Interpr√©tabilit√©** : On peut visualiser les matrices d'attention
4. **Flexibilit√©** : S'adapte √† diff√©rentes modalit√©s (texte, images, audio)

## Limitations

1. **Complexit√© quadratique** : O(n¬≤) en m√©moire et calcul pour une s√©quence de longueur n
2. **Besoin de donn√©es** : N√©cessite beaucoup de donn√©es pour l'entra√Ænement
3. **Co√ªt computationnel** : Tr√®s gourmand en ressources GPU

## √âvolutions R√©centes

Pour r√©soudre la limitation de complexit√©, plusieurs variantes ont √©t√© propos√©es :

- **Longformer** : Attention sparse pour les documents longs
- **Reformer** : Utilisation de LSH (Locality-Sensitive Hashing)
- **Linear Transformers** : Attention lin√©aire en O(n)

## Conclusion

Les Transformers ont r√©volutionn√© le machine learning et continuent d'√©voluer. Le m√©canisme d'attention, simple mais puissant, permet de capturer des relations complexes dans les donn√©es de mani√®re parall√©lisable et efficace.

Avec l'av√®nement des grands mod√®les de langage (LLM) comme GPT-4 ou Claude, les Transformers sont devenus le socle de l'intelligence artificielle moderne.

---

<div class="question-block" data-worker-url="https://rag-blog-worker.seb-sime.workers.dev/api/ask">
  <h3>üí¨ Une question sur l'article ?</h3>
  <p>Posez votre question et obtenez une r√©ponse bas√©e sur le contenu de cet article gr√¢ce au syst√®me RAG local.</p>

  <div id="rag-status">‚è≥ Initialisation du syst√®me RAG local...</div>

  <div class="question-input-wrapper">
    <input
      type="text"
      id="user-question"
      placeholder="Ex: Quelle est la diff√©rence entre Q, K et V dans l'attention ?"
      disabled
    />
    <button id="ask-button" disabled>‚è≥ Chargement...</button>
  </div>

  <div id="answer-container"></div>
</div>

---

## R√©f√©rences

- Vaswani, A., et al. (2017). "Attention is All You Need"
- Devlin, J., et al. (2018). "BERT: Pre-training of Deep Bidirectional Transformers"
- Brown, T., et al. (2020). "Language Models are Few-Shot Learners" (GPT-3)
- Dosovitskiy, A., et al. (2020). "An Image is Worth 16x16 Words: Transformers for Image Recognition"
